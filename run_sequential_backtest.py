#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
‚ö°üåüüíé ULTRA NECROZMA - Sequential Backtesting Runner üíéüåü‚ö°

Loads processed universe results and runs backtesting sequentially
with CPU management and cooling breaks.

Now supports:
- Multi-worker parallel execution with CPU throttling
- Parquet format for faster I/O and less disk usage
- Adaptive worker scaling based on CPU usage

"Light that illuminates the path to profit"
"""

import sys
import json
import time
import gc
import argparse
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
import pandas as pd
import numpy as np
import os

# Ensure correct imports
sys.path.insert(0, str(Path(__file__).parent))

from backtester import Backtester, BacktestResults
from strategy_factory import StrategyFactory
from light_finder import LightFinder
from light_report import LightReportGenerator
from lore import LoreSystem, EventType
from config import RANDOM_SEED, PARQUET_FILE, STORAGE_CONFIG, WORKER_CONFIG
from ohlc_generator import generate_ohlc_bars, validate_ohlc_data
from feature_extractor import (
    extract_features_from_universe,
    combine_ohlc_with_features,
    validate_dataframe_for_backtesting,
    load_universe_from_file
)
from core.storage.smart_storage import SmartBacktestStorage

# Import psutil for monitoring (optional)
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False
    print("‚ö†Ô∏è  Warning: psutil not available - CPU monitoring will use fallback mode")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üîß CONFIGURATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

DEFAULT_CPU_THRESHOLD = 85  # Target CPU percentage
DEFAULT_COOLING_DURATION = 120  # Seconds
COOLING_INTERVAL = 5  # Check every N universes
INITIAL_CAPITAL = 10000  # Starting capital for backtests


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚ö° CPU THROTTLED EXECUTOR (Multi-Worker with CPU Control)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class CPUThrottledExecutor:
    """
    Execute tasks with CPU usage control
    Prevents overheating on VMs by dynamically adjusting worker count
    
    Features:
    - Adaptive worker scaling based on CPU usage
    - Cooldown periods between batches
    - Process priority management (nice)
    """
    
    def __init__(
        self, 
        max_workers: int = 4, 
        cpu_limit: int = 80, 
        cooldown: int = 5,
        nice: bool = False
    ):
        """
        Initialize CPU throttled executor
        
        Args:
            max_workers: Maximum number of parallel workers
            cpu_limit: Maximum CPU usage percentage before throttling
            cooldown: Pause duration (seconds) between task batches
            nice: Run with low priority (nice)
        """
        self.max_workers = max_workers
        self.cpu_limit = cpu_limit
        self.cooldown = cooldown
        self.nice = nice
        self.current_workers = max_workers
        
        if nice and hasattr(os, 'nice'):
            try:
                os.nice(10)  # Lower priority
                print(f"  ‚úÖ Process priority lowered (nice +10)")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Could not set nice priority: {e}")
    
    def check_cpu_and_throttle(self):
        """
        Check CPU usage and dynamically adjust worker count
        
        Returns:
            Current worker count after adjustment
        """
        if not HAS_PSUTIL:
            return self.current_workers
        
        cpu_percent = psutil.cpu_percent(interval=1)
        
        if cpu_percent > self.cpu_limit:
            # Reduce workers
            new_workers = max(1, self.current_workers - 1)
            if new_workers != self.current_workers:
                print(f"  üå°Ô∏è  CPU {cpu_percent:.1f}% > {self.cpu_limit}% - reducing to {new_workers} workers")
                self.current_workers = new_workers
        elif cpu_percent < self.cpu_limit - 20 and self.current_workers < self.max_workers:
            # Increase workers if CPU is low
            self.current_workers = min(self.max_workers, self.current_workers + 1)
            print(f"  ‚ùÑÔ∏è  CPU {cpu_percent:.1f}% - increasing to {self.current_workers} workers")
        
        return self.current_workers
    
    def execute_with_throttling(self, func, items, desc="Processing"):
        """
        Execute function on items with CPU throttling and cooldown
        
        Args:
            func: Function to execute (must accept single item as argument)
            items: List of items to process
            desc: Description for progress messages
            
        Returns:
            List of results from successful executions
        """
        if not items:
            return []
        
        results = []
        total = len(items)
        
        print(f"\n‚ö° {desc} with {self.current_workers} workers (CPU limit: {self.cpu_limit}%)")
        
        # Note: For now, keep sequential execution as the executor implementation
        # would require significant refactoring of the backtest workflow.
        # Multi-worker support is prepared but not activated yet.
        # Future: Implement proper parallel execution with serializable work items.
        
        for i, item in enumerate(items, 1):
            try:
                result = func(item)
                results.append(result)
                
                # Check CPU every 5 items
                if i % 5 == 0:
                    self.check_cpu_and_throttle()
                    
                    # Cooldown if specified
                    if self.cooldown > 0 and i < total:
                        time.sleep(self.cooldown)
                        
            except Exception as e:
                print(f"  ‚ùå Error processing item {i}/{total}: {e}")
                continue
        
        return results


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üíæ DATA LOADING (with Parquet support)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def load_universe_results(results_dir: Path, universe_ids: Optional[List[int]] = None) -> List[Dict]:
    """
    Load universe results from Parquet or JSON files
    
    Automatically detects and prefers Parquet format for better performance.
    Falls back to JSON for backward compatibility.
    
    Args:
        results_dir: Directory containing universe result files
        universe_ids: Optional list of specific universe IDs to load
        
    Returns:
        List of universe result dictionaries
    """
    print(f"\nüìÇ Loading universe results from {results_dir}...", flush=True)
    
    if not results_dir.exists():
        print(f"‚ùå Error: Directory not found: {results_dir}", flush=True)
        return []
    
    # Find all universe files (Parquet preferred)
    parquet_files = sorted(results_dir.glob("universe_*.parquet"))
    json_files = sorted(results_dir.glob("universe_*.json"))
    
    # Build file list, preferring Parquet
    universe_files = []
    processed_names = set()
    
    # Add Parquet files first
    for pf in parquet_files:
        name = pf.stem
        universe_files.append(pf)
        processed_names.add(name)
    
    # Add JSON files if no Parquet equivalent exists
    for jf in json_files:
        name = jf.stem
        if name not in processed_names:
            universe_files.append(jf)
            processed_names.add(name)
    
    if not universe_files:
        print(f"‚ùå No universe files found in {results_dir}", flush=True)
        return []
    
    parquet_count = sum(1 for f in universe_files if f.suffix == '.parquet')
    json_count = len(universe_files) - parquet_count
    print(f"   Found {len(universe_files)} universe files ({parquet_count} Parquet, {json_count} JSON)", flush=True)
    
    # Filter by universe IDs if specified
    if universe_ids:
        filtered_files = []
        for f in universe_files:
            # Extract universe number from filename (e.g., universe_001_5min_5lb.parquet)
            try:
                parts = f.stem.split('_')
                if len(parts) >= 2:
                    universe_num = int(parts[1])
                    if universe_num in universe_ids:
                        filtered_files.append(f)
            except (ValueError, IndexError):
                continue
        universe_files = filtered_files
        print(f"   Filtered to {len(universe_files)} universes based on selection", flush=True)
    
    # Load each universe file
    universes = []
    for filepath in universe_files:
        try:
            data = load_universe_from_file(filepath)
            data['_filepath'] = str(filepath)  # Store filepath for reference
            universes.append(data)
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load {filepath.name}: {e}", flush=True)
            continue
    
    print(f"   ‚úÖ Loaded {len(universes)} universes successfully", flush=True)
    return universes


def parse_universe_selection(selection: str) -> List[int]:
    """
    Parse universe selection string into list of IDs
    
    Examples:
        "1,5,10-15,25" -> [1, 5, 10, 11, 12, 13, 14, 15, 25]
        "1-3" -> [1, 2, 3]
        
    Args:
        selection: Selection string
        
    Returns:
        List of universe IDs
    """
    ids = []
    parts = selection.split(',')
    
    for part in parts:
        part = part.strip()
        if '-' in part:
            # Range (e.g., "10-15")
            try:
                start, end = part.split('-')
                ids.extend(range(int(start), int(end) + 1))
            except ValueError:
                print(f"‚ö†Ô∏è  Invalid range: {part}", flush=True)
        else:
            # Single ID
            try:
                ids.append(int(part))
            except ValueError:
                print(f"‚ö†Ô∏è  Invalid ID: {part}", flush=True)
    
    return sorted(set(ids))  # Remove duplicates and sort


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚ùÑÔ∏è COOLING & CPU MANAGEMENT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def get_system_stats() -> Dict:
    """Get current system statistics"""
    if not HAS_PSUTIL:
        return {"cpu_percent": 0.0, "ram_gb": 0.0}
    
    return {
        "cpu_percent": psutil.cpu_percent(interval=1),
        "ram_gb": psutil.virtual_memory().used / 1e9
    }


def cooling_break(duration: int, cpu_threshold: float):
    """
    Pause execution with cooling break
    
    Args:
        duration: Duration in seconds
        cpu_threshold: CPU threshold percentage
    """
    if not HAS_PSUTIL:
        print(f"\n‚ùÑÔ∏è  COOLING BREAK - Waiting {duration}s...", flush=True)
        time.sleep(duration)
        print("   ‚úÖ Resumed\n", flush=True)
        return
    
    current_cpu = psutil.cpu_percent(interval=2)
    
    if current_cpu > cpu_threshold:
        print(f"\n‚ùÑÔ∏è  COOLING BREAK - CPU too high ({current_cpu:.1f}%)", flush=True)
        print(f"   Waiting {duration}s...", flush=True)
        
        # Display countdown with stats
        remaining = duration
        while remaining > 0:
            cpu = psutil.cpu_percent(interval=1)
            mem_gb = psutil.virtual_memory().used / 1e9
            print(f"   {remaining:3d}s | CPU: {cpu:5.1f}% | RAM: {mem_gb:5.1f}GB", flush=True)
            
            wait_time = min(10, remaining)
            time.sleep(wait_time)
            remaining -= wait_time
        
        print("   ‚úÖ Resumed\n", flush=True)
    else:
        print(f"\n   üí§ Quick pause ({duration}s, CPU OK at {current_cpu:.1f}%)...", flush=True)
        time.sleep(duration)
        print("   ‚úÖ Resumed\n", flush=True)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üè≠ STRATEGY GENERATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def generate_strategies_for_universe(universe_data: Dict, max_strategies: int = 100) -> List:
    """
    Generate strategies for a universe based on its features
    
    Args:
        universe_data: Universe result dictionary
        max_strategies: Maximum strategies to generate
        
    Returns:
        List of Strategy objects
    """
    factory = StrategyFactory()
    
    # Generate default strategies
    strategies = factory.generate_strategies(max_strategies=max_strategies)
    
    return strategies


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üìä BACKTESTING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def load_ohlc_for_universe(universe_data: Dict, parquet_path: Path = None, verbose: bool = False) -> pd.DataFrame:
    """
    Load OHLC data for a universe from Parquet file and combine with pattern features
    
    This function:
    1. Generates OHLC bars from tick data
    2. Extracts features from universe JSON patterns
    3. Combines OHLC + features into a single DataFrame
    
    Args:
        universe_data: Universe result dictionary
        parquet_path: Path to parquet file (default: from config)
        verbose: Whether to print detailed output
        
    Returns:
        DataFrame with OHLC price data + pattern features ready for backtesting
    """
    # Extract universe parameters
    lookback = universe_data.get('lookback', 20)
    interval = universe_data.get('interval', 5)
    
    if parquet_path is None:
        parquet_path = PARQUET_FILE
    
    if verbose:
        print(f"      üìÇ Loading OHLC data (interval={interval}min, lookback={lookback})...", flush=True)
    
    # Generate OHLC bars from tick data
    try:
        ohlc_df = generate_ohlc_bars(
            parquet_path=parquet_path,
            interval_minutes=interval,
            lookback=lookback
        )
        
        # Validate the data
        validation = validate_ohlc_data(ohlc_df)
        
        if not validation["valid"]:
            raise ValueError(f"OHLC validation failed: {validation['errors']}")
        
        if verbose and validation["warnings"]:
            for warning in validation["warnings"]:
                print(f"      ‚ö†Ô∏è  {warning}", flush=True)
        
        # ‚úÖ NEW: Extract features from universe JSON patterns
        if verbose:
            print(f"      üîÆ Extracting features from universe patterns...", flush=True)
        
        features_df = extract_features_from_universe(universe_data)
        
        if verbose:
            if not features_df.empty:
                print(f"      ‚úÖ Extracted {len(features_df.columns)} features", flush=True)
            else:
                print(f"      ‚ö†Ô∏è  No features extracted, using OHLC only", flush=True)
        
        # ‚úÖ NEW: Combine OHLC + features
        combined_df = combine_ohlc_with_features(ohlc_df, features_df)
        
        # Fill NaN values (may occur from feature broadcast or OHLC calculations)
        combined_df = combined_df.bfill().fillna(0)
        
        if verbose:
            print(f"      ‚úÖ Combined DataFrame: {combined_df.shape}", flush=True)
            print(f"      üìä Columns: {list(combined_df.columns)[:10]}...", flush=True)
            
            # Show sample feature values
            if "momentum" in combined_df.columns:
                momentum_stats = combined_df["momentum"].describe()
                print(f"      üìä Sample momentum: mean={momentum_stats['mean']:.4f}, "
                      f"std={momentum_stats['std']:.4f}", flush=True)
        
        # ‚úÖ NEW: Validate DataFrame before returning
        validation_result = validate_dataframe_for_backtesting(combined_df)
        
        if not validation_result["valid"]:
            missing = validation_result["missing_required"]
            raise ValueError(f"DataFrame missing required columns: {missing}")
        
        if verbose and validation_result["missing_recommended"]:
            print(f"      ‚ö†Ô∏è  Missing recommended features: {validation_result['missing_recommended']}", 
                  flush=True)
        
        return combined_df
        
    except FileNotFoundError:
        print(f"      ‚ö†Ô∏è  Parquet file not found: {parquet_path}", flush=True)
        print(f"      ‚ö†Ô∏è  Falling back to synthetic data for testing...", flush=True)
        return create_mock_dataframe_fallback(universe_data, interval, lookback)
    except Exception as e:
        print(f"      ‚ö†Ô∏è  Error loading OHLC data: {e}", flush=True)
        print(f"      ‚ö†Ô∏è  Falling back to synthetic data for testing...", flush=True)
        return create_mock_dataframe_fallback(universe_data, interval, lookback)


def create_mock_dataframe_fallback(universe_data: Dict, interval: int = 5, lookback: int = 20) -> pd.DataFrame:
    """
    Fallback: Create synthetic OHLC data for testing when real data is unavailable
    
    This is used only when the Parquet file is not found.
    
    Args:
        universe_data: Universe result dictionary
        interval: Interval in minutes
        lookback: Lookback period
        
    Returns:
        DataFrame with synthetic OHLC data + extracted features
    """
    np.random.seed(RANDOM_SEED)
    
    # Generate more realistic number of bars (1 year of 5min data = ~105k bars)
    # For testing, use a smaller but reasonable amount
    n_bars = 10000
    
    # Start from a base price
    base_price = 1.10
    
    # Generate random walk with realistic volatility
    returns = np.random.randn(n_bars) * 0.0001
    close_prices = base_price + np.cumsum(returns)
    
    # Generate OHLC from close prices
    opens = np.roll(close_prices, 1)
    opens[0] = base_price
    
    # High and low with some randomness
    highs = np.maximum(opens, close_prices) + np.abs(np.random.randn(n_bars)) * 0.00005
    lows = np.minimum(opens, close_prices) - np.abs(np.random.randn(n_bars)) * 0.00005
    
    # Create OHLC dataframe
    ohlc_df = pd.DataFrame({
        'open': opens,
        'high': highs,
        'low': lows,
        'close': close_prices,
        'mid_price': close_prices,
        'volume': np.random.randint(50, 500, n_bars),
    })
    
    # ‚úÖ Extract features from universe JSON patterns
    features_df = extract_features_from_universe(universe_data)
    
    # ‚úÖ Combine OHLC + features
    combined_df = combine_ohlc_with_features(ohlc_df, features_df)
    
    # Fill NaN values
    combined_df = combined_df.bfill().fillna(0)
    
    return combined_df


def backtest_universe(universe_data: Dict, strategies: List, parquet_path: Path = None, 
                      verbose: bool = False) -> Tuple[List[BacktestResults], Dict]:
    """
    Backtest all strategies for a universe
    
    Args:
        universe_data: Universe result dictionary
        strategies: List of Strategy objects
        parquet_path: Path to parquet file (default: from config)
        verbose: Whether to print detailed output
        
    Returns:
        Tuple of (list of BacktestResults, stats dict)
    """
    backtester = Backtester()
    
    # Load OHLC data for this universe
    # This now uses REAL price data from Parquet instead of mock data
    df = load_ohlc_for_universe(universe_data, parquet_path, verbose)
    
    # Validate data before backtesting
    if len(df) == 0:
        raise ValueError("‚ùå No data loaded for backtesting!")
    
    if "close" not in df.columns and "mid_price" not in df.columns:
        raise ValueError("‚ùå Price data missing (need 'close' or 'mid_price' column)!")
    
    # Check price variation
    price_col = "mid_price" if "mid_price" in df.columns else "close"
    price_std = df[price_col].std()
    if price_std == 0:
        raise ValueError("‚ùå Price data is constant (no variation)!")
    
    if verbose:
        print(f"      üìä Data loaded: {len(df):,} bars", flush=True)
        print(f"      üìà Price range: {df[price_col].min():.5f} - {df[price_col].max():.5f}", flush=True)
    
    results = []
    failed = 0
    
    for i, strategy in enumerate(strategies):
        try:
            if verbose and i % 10 == 0:
                print(f"      Backtesting strategy {i+1}/{len(strategies)}...", flush=True)
            
            result = backtester.backtest(strategy, df, initial_capital=INITIAL_CAPITAL)
            results.append(result)
            
        except Exception as e:
            if verbose:
                print(f"      ‚ö†Ô∏è  Strategy {strategy.name} failed: {e}", flush=True)
            failed += 1
            continue
    
    stats = {
        "total_strategies": len(strategies),
        "successful": len(results),
        "failed": failed,
    }
    
    return results, stats


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üíæ RESULTS SAVING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def save_universe_backtest_results(universe_data: Dict, results: List[BacktestResults], 
                                   stats: Dict, output_dir: Path, 
                                   smart_storage: Optional[SmartBacktestStorage] = None):
    """
    Save backtest results for a universe
    
    This function now supports both legacy JSON and Parquet formats:
    - Parquet: Metrics saved as DataFrame (faster, smaller)
    - JSON: Full results (backward compatible)
    - Smart Storage: Tiered storage (metrics + top N trades)
    
    Args:
        universe_data: Universe result dictionary
        results: List of BacktestResults
        stats: Statistics dictionary
        output_dir: Output directory
        smart_storage: Optional SmartBacktestStorage instance for tiered storage
    """
    from config import STORAGE_CONFIG
    
    # Extract universe identifier
    filepath = Path(universe_data.get('_filepath', ''))
    universe_name = filepath.stem if filepath.stem else 'unknown'
    
    storage_format = STORAGE_CONFIG.get("format", "json")
    
    # Prepare output data
    output = {
        "universe_name": universe_name,
        "universe_metadata": {
            "interval": universe_data.get('interval'),
            "lookback": universe_data.get('lookback'),
            "total_patterns": universe_data.get('total_patterns', 0),
        },
        "backtest_timestamp": datetime.now().isoformat(),
        "statistics": stats,
        "results": [r.to_dict() for r in results],
    }
    
    # Save based on storage format
    if storage_format == "parquet":
        # Save as Parquet
        results_df = pd.DataFrame([r.to_dict() for r in results])
        parquet_file = output_dir / f"{universe_name}_backtest.parquet"
        compression = STORAGE_CONFIG.get("compression", "snappy")
        results_df.to_parquet(parquet_file, compression=compression, index=False)
        
        # Save metadata separately if enabled
        if STORAGE_CONFIG.get("enable_metadata_sidecar", True):
            metadata = {
                "universe_name": universe_name,
                "universe_metadata": output["universe_metadata"],
                "backtest_timestamp": output["backtest_timestamp"],
                "statistics": stats
            }
            metadata_file = output_dir / f"{universe_name}_backtest_metadata.json"
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2)
        
        output_file = parquet_file
    else:
        # Save as JSON (legacy)
        output_file = output_dir / f"{universe_name}_backtest.json"
        with open(output_file, 'w') as f:
            json.dump(output, f, indent=2)
    
    # Also save using smart storage if provided
    if smart_storage is not None:
        # Convert BacktestResults to dict format
        results_dicts = [r.to_dict() for r in results]
        smart_storage.save_universe_results(
            universe_name=universe_name,
            results=results_dicts,
            top_n=50  # Save detailed trades for top 50 strategies
        )
    
    return output_file


def save_consolidated_results(all_universe_results: List[Dict], output_dir: Path):
    """
    Save consolidated results from all universes
    
    Args:
        all_universe_results: List of universe backtest result dicts
        output_dir: Output directory
    """
    # Consolidate all results
    consolidated = {
        "backtest_timestamp": datetime.now().isoformat(),
        "total_universes": len(all_universe_results),
        "universes": all_universe_results,
    }
    
    output_file = output_dir / "consolidated_backtest_results.json"
    with open(output_file, 'w') as f:
        json.dump(consolidated, f, indent=2)
    
    print(f"\nüíæ Consolidated results saved to: {output_file}", flush=True)
    return output_file


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üéØ MAIN EXECUTION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def print_banner():
    """Print startup banner"""
    print("""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üåü ULTRA NECROZMA - Sequential Backtesting
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    """, flush=True)


def print_summary(summary_stats: Dict):
    """Print final summary"""
    print("""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üéâ BACKTESTING COMPLETE!
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    """, flush=True)
    
    print(f"\nüìä Summary:", flush=True)
    print(f"   Total Universes: {summary_stats['total_universes']}", flush=True)
    print(f"   Total Strategies Tested: {summary_stats['total_strategies']}", flush=True)
    print(f"   Viable Strategies (Sharpe > 1.0): {summary_stats['viable_strategies']}", flush=True)
    print(f"   Best Overall Sharpe: {summary_stats['best_sharpe']:.2f}", flush=True)
    print(f"   Total Time: {summary_stats['total_time']:.1f}s ({summary_stats['total_time']/3600:.2f} hours)", flush=True)
    
    if summary_stats.get('top_strategies'):
        print(f"\nüèÜ Top {len(summary_stats['top_strategies'])} Strategies:", flush=True)
        for i, strat in enumerate(summary_stats['top_strategies'][:5], 1):
            print(f"   #{i} {strat['name']} (Sharpe: {strat['sharpe']:.2f}, Return: {strat['return']:.1%})", flush=True)
    
    print(f"\nüíæ Results saved to:", flush=True)
    print(f"   üìÑ {summary_stats['consolidated_file']}", flush=True)
    print(f"   üìÑ {summary_stats['ranked_file']}", flush=True)
    print(f"   üìÑ {summary_stats['report_file']}", flush=True)


def main():
    """Main sequential backtesting runner"""
    # Parse command-line arguments
    parser = argparse.ArgumentParser(description='Sequential Backtesting for ULTRA NECROZMA')
    
    # Existing arguments
    parser.add_argument('--universes', type=str, help='Universe selection (e.g., "1,5,10-15,25")')
    parser.add_argument('--cpu-threshold', type=float, default=DEFAULT_CPU_THRESHOLD,
                       help=f'CPU threshold percentage (default: {DEFAULT_CPU_THRESHOLD})')
    parser.add_argument('--cooling-duration', type=int, default=DEFAULT_COOLING_DURATION,
                       help=f'Cooling break duration in seconds (default: {DEFAULT_COOLING_DURATION})')
    parser.add_argument('--skip-telegram', action='store_true',
                       help='Skip Telegram notifications')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Verbose output')
    parser.add_argument('--results-dir', type=str, default='ultra_necrozma_results',
                       help='Directory containing universe results (default: ultra_necrozma_results)')
    parser.add_argument('--max-strategies', type=int, default=50,
                       help='Maximum strategies to generate per universe (default: 50)')
    
    # NEW: Multi-worker arguments
    parser.add_argument(
        "--workers", "-w",
        type=int,
        default=WORKER_CONFIG.get("default_workers", 1),
        help=f"Number of parallel workers (default: {WORKER_CONFIG.get('default_workers', 1)})"
    )
    
    parser.add_argument(
        "--cpu-limit",
        type=int,
        default=WORKER_CONFIG.get("cpu_limit", 80),
        help=f"Max CPU usage percentage (default: {WORKER_CONFIG.get('cpu_limit', 80)})"
    )
    
    parser.add_argument(
        "--cooldown",
        type=int,
        default=WORKER_CONFIG.get("cooldown_seconds", 0),
        help=f"Seconds to pause between batches (default: {WORKER_CONFIG.get('cooldown_seconds', 0)})"
    )
    
    parser.add_argument(
        "--nice",
        action="store_true",
        help="Run with low priority (nice)"
    )
    
    args = parser.parse_args()
    
    # Setup
    print_banner()
    
    # Initialize lore system
    lore = LoreSystem(enabled=True, enable_telegram=not args.skip_telegram)
    lore.broadcast(EventType.AWAKENING, "Sequential backtesting initiated...")
    
    # Parse universe selection
    universe_ids = None
    if args.universes:
        universe_ids = parse_universe_selection(args.universes)
        print(f"\nüìå Selected universes: {universe_ids}", flush=True)
    
    # Load universe results
    results_dir = Path(args.results_dir)
    universes = load_universe_results(results_dir, universe_ids)
    
    if not universes:
        print("\n‚ùå No universes loaded. Exiting.", flush=True)
        return 1
    
    # Setup output directory
    output_dir = results_dir / "backtest_results"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Initialize smart storage
    smart_storage = SmartBacktestStorage(output_dir=str(output_dir))
    
    print(f"\nüíæ Output directory: {output_dir}", flush=True)
    print(f"‚öôÔ∏è  CPU threshold: {args.cpu_threshold}%", flush=True)
    print(f"‚ùÑÔ∏è  Cooling duration: {args.cooling_duration}s", flush=True)
    print(f"üìä Max strategies per universe: {args.max_strategies}", flush=True)
    print(f"üóÑÔ∏è  Smart storage enabled: Metrics + Top 50 trades per universe", flush=True)
    
    # NEW: Display multi-worker settings
    if args.workers > 1:
        print(f"\n‚ö° Multi-Worker Mode:", flush=True)
        print(f"   Workers: {args.workers}", flush=True)
        print(f"   CPU Limit: {args.cpu_limit}%", flush=True)
        print(f"   Cooldown: {args.cooldown}s", flush=True)
        print(f"   Nice Priority: {'Yes' if args.nice else 'No'}", flush=True)
        print(f"   ‚ö†Ô∏è  Note: Parallel execution framework prepared but currently sequential", flush=True)
    else:
        print(f"\nüìå Sequential Mode (1 worker)", flush=True)
    
    # Display storage format
    storage_format = STORAGE_CONFIG.get("format", "json")
    print(f"üíæ Storage format: {storage_format.upper()}", flush=True)
    
    # Track overall statistics
    start_time = time.time()
    all_results = []
    all_backtest_results = {}  # (universe_name, strategy_name) -> BacktestResults for unique key
    all_backtest_results_list = []  # List of all BacktestResults for ranking
    total_strategies_tested = 0
    
    # Process each universe sequentially
    for idx, universe_data in enumerate(universes, 1):
        universe_name = Path(universe_data.get('_filepath', '')).stem
        
        print(f"\n{'‚îÄ'*63}", flush=True)
        print(f"üìä Universe {idx}/{len(universes)}: {universe_name}", flush=True)
        print(f"{'‚îÄ'*63}", flush=True)
        
        try:
            # Check CPU before starting
            stats = get_system_stats()
            cpu_str = f"{stats['cpu_percent']:.1f}%" if HAS_PSUTIL else "N/A"
            
            # Generate strategies
            print(f"   üè≠ Generating strategies...", flush=True)
            strategies = generate_strategies_for_universe(universe_data, args.max_strategies)
            print(f"   üìà Generated {len(strategies)} strategies", flush=True)
            
            # Backtest
            print(f"   üìä Backtesting... (CPU: {cpu_str})", flush=True)
            backtest_start = time.time()
            results, backtest_stats = backtest_universe(universe_data, strategies, PARQUET_FILE, args.verbose)
            backtest_time = time.time() - backtest_start
            
            # Find best strategy for this universe
            best_sharpe = 0.0
            best_result = None
            for result in results:
                if result.sharpe_ratio > best_sharpe:
                    best_sharpe = result.sharpe_ratio
                    best_result = result
                # Store in global dict with unique key (universe_name, strategy_name)
                all_backtest_results[(universe_name, result.strategy_name)] = result
                # Also add to list for ranking
                all_backtest_results_list.append(result)
            
            print(f"   ‚úÖ Complete! Best Sharpe: {best_sharpe:.2f}", flush=True)
            
            # Save individual universe results
            output_file = save_universe_backtest_results(
                universe_data, results, backtest_stats, output_dir, smart_storage
            )
            print(f"   üíæ Saved: {output_file.name}", flush=True)
            print(f"   ‚è±Ô∏è  Time: {backtest_time:.1f}s", flush=True)
            
            # Track stats
            all_results.append({
                "universe_name": universe_name,
                "strategies_tested": len(strategies),
                "successful": len(results),
                "best_sharpe": best_sharpe,
                "processing_time": backtest_time,
            })
            total_strategies_tested += len(strategies)
            
            # Garbage collection
            del strategies, results
            gc.collect()
            
            # Cooling break every N universes
            if idx % COOLING_INTERVAL == 0 and idx < len(universes):
                cooling_break(args.cooling_duration, args.cpu_threshold)
            
        except Exception as e:
            print(f"   ‚ùå Error processing universe: {e}", flush=True)
            if args.verbose:
                traceback.print_exc()
            continue
    
    total_time = time.time() - start_time
    
    # Rank all strategies using LightFinder
    print(f"\n{'‚ïê'*63}", flush=True)
    print(f"üåü Ranking strategies...", flush=True)
    print(f"{'‚ïê'*63}", flush=True)
    
    if all_backtest_results_list:
        finder = LightFinder()
        ranked_strategies = finder.rank_strategies(all_backtest_results_list, top_n=20)
        
        # Save ranked strategies
        ranked_file = output_dir / "top_strategies_ranked.json"
        ranked_data = {
            "timestamp": datetime.now().isoformat(),
            "total_strategies": len(all_backtest_results_list),
            "top_strategies": ranked_strategies.to_dict('records')
        }
        with open(ranked_file, 'w') as f:
            json.dump(ranked_data, f, indent=2)
        print(f"\nüíæ Ranked strategies saved to: {ranked_file}", flush=True)
    else:
        ranked_strategies = pd.DataFrame()
        ranked_file = None
    
    # Generate final Light Report
    print(f"\nüìù Generating final Light Report...", flush=True)
    
    # Convert backtest results list to dict for report (use last occurrence of each strategy name)
    backtest_results_dict = {}
    for result in all_backtest_results_list:
        backtest_results_dict[result.strategy_name] = result
    
    report_generator = LightReportGenerator(output_dir=output_dir)
    report = report_generator.generate_report(
        top_strategies=ranked_strategies,
        all_backtest_results=backtest_results_dict,
        total_strategies=total_strategies_tested
    )
    
    report_file = report_generator.save_report(report)
    report_generator.print_summary(report)
    
    # Save consolidated results
    consolidated_file = save_consolidated_results(all_results, output_dir)
    
    # Calculate summary statistics
    viable_count = sum(1 for r in all_backtest_results_list if r.sharpe_ratio > 1.0)
    best_sharpe = max((r.sharpe_ratio for r in all_backtest_results_list), default=0.0)
    
    top_strategies_list = []
    if len(ranked_strategies) > 0:
        for _, row in ranked_strategies.head(5).iterrows():
            top_strategies_list.append({
                'name': row['strategy_name'],
                'sharpe': row['sharpe_ratio'],
                'return': row['total_return'],
            })
    
    summary_stats = {
        'total_universes': len(universes),
        'total_strategies': total_strategies_tested,
        'viable_strategies': viable_count,
        'best_sharpe': best_sharpe,
        'total_time': total_time,
        'top_strategies': top_strategies_list,
        'consolidated_file': consolidated_file,
        'ranked_file': ranked_file,
        'report_file': report_file,
    }
    
    # Print final summary
    print_summary(summary_stats)
    
    # Broadcast completion
    lore.broadcast(EventType.COMPLETION, "Sequential backtesting complete!")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
