#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
âš¡ðŸŒŸðŸ’Ž ULTRA NECROZMA - JSON to Parquet Migration ðŸ’ŽðŸŒŸâš¡

Migrate existing JSON files to Parquet format
Reduces disk usage by ~85% and improves read speed by 20x

"From JSON's glow to Parquet's flow - efficiency shall grow"
"""

import json
import pandas as pd
from pathlib import Path
import argparse
from typing import Dict, List, Optional
import sys


def convert_universe_to_dataframe(universe_data: dict) -> pd.DataFrame:
    """
    Convert universe JSON structure to flat DataFrame
    
    Handles nested structure:
    - results â†’ level â†’ direction â†’ feature_stats
    
    Args:
        universe_data: Universe dictionary from JSON
        
    Returns:
        DataFrame with flattened feature statistics
    """
    rows = []
    
    results = universe_data.get('results', {})
    for level_name, level_data in results.items():
        if not isinstance(level_data, dict):
            continue
        for direction, direction_data in level_data.items():
            if not isinstance(direction_data, dict):
                continue
            
            # Extract feature_stats
            feature_stats = direction_data.get('feature_stats', {})
            
            # Create row with level and direction
            row = {
                'level': level_name,
                'direction': direction,
            }
            
            # Add all feature stats
            for key, value in feature_stats.items():
                if isinstance(value, (int, float, str, bool)):
                    row[key] = value
            
            # Add pattern count if available
            if 'pattern_count' in direction_data:
                row['pattern_count'] = direction_data['pattern_count']
            
            rows.append(row)
    
    return pd.DataFrame(rows)


def migrate_universes(input_dir: Path, output_dir: Path, delete_json: bool = False) -> Dict:
    """
    Migrate universe JSON files to Parquet
    
    Args:
        input_dir: Directory with universe_*.json files
        output_dir: Output directory for .parquet files
        delete_json: Delete original JSON after successful conversion
        
    Returns:
        Dictionary with migration statistics
    """
    json_files = list(input_dir.glob("universe_*.json"))
    
    if not json_files:
        print(f"âŒ No universe JSON files found in {input_dir}")
        return {
            'files_processed': 0,
            'total_size_before_mb': 0,
            'total_size_after_mb': 0,
            'savings_pct': 0
        }
    
    print(f"ðŸ“ Found {len(json_files)} universe JSON files to migrate")
    
    total_json_size = 0
    total_parquet_size = 0
    files_processed = 0
    files_failed = 0
    
    for json_path in json_files:
        try:
            # Load JSON
            with open(json_path, 'r') as f:
                data = json.load(f)
            
            # Convert to DataFrame
            df = convert_universe_to_dataframe(data)
            
            if df.empty:
                print(f"  âš ï¸  {json_path.name}: No data to convert, skipping")
                continue
            
            # Save as Parquet
            parquet_path = output_dir / json_path.name.replace('.json', '.parquet')
            df.to_parquet(parquet_path, compression='snappy', index=False)
            
            # Save metadata separately (small file)
            metadata = {
                'name': data.get('name', ''),
                'config': data.get('config', {}),
                'processing_time': data.get('processing_time', 0),
                'total_patterns': data.get('total_patterns', 0),
                'metadata': data.get('metadata', {})
            }
            metadata_path = output_dir / json_path.name.replace('.json', '_metadata.json')
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            # Calculate savings
            json_size = json_path.stat().st_size / 1e6
            parquet_size = parquet_path.stat().st_size / 1e6
            metadata_size = metadata_path.stat().st_size / 1e6
            total_new_size = parquet_size + metadata_size
            savings = (1 - total_new_size/json_size) * 100 if json_size > 0 else 0
            
            total_json_size += json_size
            total_parquet_size += total_new_size
            files_processed += 1
            
            print(f"  âœ… {json_path.name}: {json_size:.1f}MB â†’ {total_new_size:.1f}MB ({savings:.0f}% savings)")
            
            # Delete JSON if requested
            if delete_json:
                json_path.unlink()
                print(f"     ðŸ—‘ï¸  Deleted {json_path.name}")
                
        except Exception as e:
            print(f"  âŒ {json_path.name}: Error - {e}")
            files_failed += 1
    
    total_savings = (1 - total_parquet_size/total_json_size) * 100 if total_json_size > 0 else 0
    
    print(f"\nðŸ“Š Universe Migration Complete!")
    print(f"   Files processed: {files_processed}")
    print(f"   Files failed: {files_failed}")
    print(f"   Total size before: {total_json_size:.1f}MB")
    print(f"   Total size after: {total_parquet_size:.1f}MB")
    print(f"   Total savings: {total_savings:.0f}%")
    
    return {
        'files_processed': files_processed,
        'files_failed': files_failed,
        'total_size_before_mb': total_json_size,
        'total_size_after_mb': total_parquet_size,
        'savings_pct': total_savings
    }


def migrate_backtest_results(input_dir: Path, output_dir: Path, delete_json: bool = False) -> Dict:
    """
    Migrate backtest result JSON files to Parquet
    
    Args:
        input_dir: Directory with *_backtest.json files
        output_dir: Output directory for .parquet files
        delete_json: Delete original JSON after successful conversion
        
    Returns:
        Dictionary with migration statistics
    """
    json_files = list(input_dir.glob("*_backtest.json"))
    
    if not json_files:
        print(f"âŒ No backtest JSON files found in {input_dir}")
        return {
            'files_processed': 0,
            'total_size_before_mb': 0,
            'total_size_after_mb': 0,
            'savings_pct': 0
        }
    
    print(f"ðŸ“ Found {len(json_files)} backtest JSON files to migrate")
    
    total_json_size = 0
    total_parquet_size = 0
    files_processed = 0
    files_failed = 0
    
    for json_path in json_files:
        try:
            # Load JSON
            with open(json_path, 'r') as f:
                data = json.load(f)
            
            # Extract results
            results = data.get('results', [])
            
            if not results:
                print(f"  âš ï¸  {json_path.name}: No results to convert, skipping")
                continue
            
            # Convert to DataFrame
            df = pd.DataFrame(results)
            
            # Save as Parquet
            parquet_path = output_dir / json_path.name.replace('.json', '.parquet')
            df.to_parquet(parquet_path, compression='snappy', index=False)
            
            # Save metadata separately
            metadata = {
                'universe_name': data.get('universe_name', ''),
                'universe_metadata': data.get('universe_metadata', {}),
                'backtest_timestamp': data.get('backtest_timestamp', ''),
                'statistics': data.get('statistics', {})
            }
            metadata_path = output_dir / json_path.name.replace('.json', '_metadata.json')
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            # Calculate savings
            json_size = json_path.stat().st_size / 1e6
            parquet_size = parquet_path.stat().st_size / 1e6
            metadata_size = metadata_path.stat().st_size / 1e6
            total_new_size = parquet_size + metadata_size
            savings = (1 - total_new_size/json_size) * 100 if json_size > 0 else 0
            
            total_json_size += json_size
            total_parquet_size += total_new_size
            files_processed += 1
            
            print(f"  âœ… {json_path.name}: {json_size:.1f}MB â†’ {total_new_size:.1f}MB ({savings:.0f}% savings)")
            
            # Delete JSON if requested
            if delete_json:
                json_path.unlink()
                print(f"     ðŸ—‘ï¸  Deleted {json_path.name}")
                
        except Exception as e:
            print(f"  âŒ {json_path.name}: Error - {e}")
            files_failed += 1
    
    total_savings = (1 - total_parquet_size/total_json_size) * 100 if total_json_size > 0 else 0
    
    print(f"\nðŸ“Š Backtest Results Migration Complete!")
    print(f"   Files processed: {files_processed}")
    print(f"   Files failed: {files_failed}")
    print(f"   Total size before: {total_json_size:.1f}MB")
    print(f"   Total size after: {total_parquet_size:.1f}MB")
    print(f"   Total savings: {total_savings:.0f}%")
    
    return {
        'files_processed': files_processed,
        'files_failed': files_failed,
        'total_size_before_mb': total_json_size,
        'total_size_after_mb': total_parquet_size,
        'savings_pct': total_savings
    }


def migrate_trade_logs(input_dir: Path, output_dir: Path, delete_json: bool = False) -> Dict:
    """
    Migrate trade log JSON files to Parquet
    
    Args:
        input_dir: Directory with trade log JSON files
        output_dir: Output directory for .parquet files
        delete_json: Delete original JSON after successful conversion
        
    Returns:
        Dictionary with migration statistics
    """
    # Look for trade log files in detailed_trades subdirectory
    trades_dir = input_dir / "detailed_trades"
    
    if not trades_dir.exists():
        print(f"âš ï¸  No detailed_trades directory found in {input_dir}")
        return {
            'files_processed': 0,
            'total_size_before_mb': 0,
            'total_size_after_mb': 0,
            'savings_pct': 0
        }
    
    json_files = list(trades_dir.glob("*.json"))
    
    if not json_files:
        print(f"âŒ No trade log JSON files found in {trades_dir}")
        return {
            'files_processed': 0,
            'total_size_before_mb': 0,
            'total_size_after_mb': 0,
            'savings_pct': 0
        }
    
    print(f"ðŸ“ Found {len(json_files)} trade log JSON files to migrate")
    
    # Create output directory for parquet trade logs
    output_trades_dir = output_dir / "detailed_trades"
    output_trades_dir.mkdir(parents=True, exist_ok=True)
    
    total_json_size = 0
    total_parquet_size = 0
    files_processed = 0
    files_failed = 0
    
    for json_path in json_files:
        try:
            # Load JSON
            with open(json_path, 'r') as f:
                data = json.load(f)
            
            # Extract trades
            trades = data.get('trades_detailed', [])
            
            if not trades:
                print(f"  âš ï¸  {json_path.name}: No trades to convert, skipping")
                continue
            
            # Convert to DataFrame
            df = pd.DataFrame(trades)
            
            # Save as Parquet
            parquet_path = output_trades_dir / json_path.name.replace('.json', '.parquet')
            df.to_parquet(parquet_path, compression='snappy', index=False)
            
            # Calculate savings
            json_size = json_path.stat().st_size / 1e6
            parquet_size = parquet_path.stat().st_size / 1e6
            savings = (1 - parquet_size/json_size) * 100 if json_size > 0 else 0
            
            total_json_size += json_size
            total_parquet_size += parquet_size
            files_processed += 1
            
            print(f"  âœ… {json_path.name}: {json_size:.1f}MB â†’ {parquet_size:.1f}MB ({savings:.0f}% savings)")
            
            # Delete JSON if requested
            if delete_json:
                json_path.unlink()
                print(f"     ðŸ—‘ï¸  Deleted {json_path.name}")
                
        except Exception as e:
            print(f"  âŒ {json_path.name}: Error - {e}")
            files_failed += 1
    
    total_savings = (1 - total_parquet_size/total_json_size) * 100 if total_json_size > 0 else 0
    
    print(f"\nðŸ“Š Trade Logs Migration Complete!")
    print(f"   Files processed: {files_processed}")
    print(f"   Files failed: {files_failed}")
    print(f"   Total size before: {total_json_size:.1f}MB")
    print(f"   Total size after: {total_parquet_size:.1f}MB")
    print(f"   Total savings: {total_savings:.0f}%")
    
    return {
        'files_processed': files_processed,
        'files_failed': files_failed,
        'total_size_before_mb': total_json_size,
        'total_size_after_mb': total_parquet_size,
        'savings_pct': total_savings
    }


def main():
    """Main migration script"""
    parser = argparse.ArgumentParser(
        description="Migrate JSON to Parquet - Reduce disk usage by ~85%",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Migrate universes only
  python migrate_to_parquet.py --input ultra_necrozma_results/universes
  
  # Migrate backtest results
  python migrate_to_parquet.py --input ultra_necrozma_results/backtest_results --type backtest
  
  # Migrate everything
  python migrate_to_parquet.py --all
  
  # Migrate and delete JSON files
  python migrate_to_parquet.py --all --delete-json
        """
    )
    
    parser.add_argument(
        "--input", 
        type=str, 
        default="ultra_necrozma_results/universes",
        help="Input directory containing JSON files (default: ultra_necrozma_results/universes)"
    )
    
    parser.add_argument(
        "--output", 
        type=str, 
        default=None,
        help="Output directory for Parquet files (default: same as input)"
    )
    
    parser.add_argument(
        "--type",
        choices=['universe', 'backtest', 'trades'],
        default='universe',
        help="Type of files to migrate (default: universe)"
    )
    
    parser.add_argument(
        "--delete-json", 
        action="store_true",
        help="Delete JSON files after successful conversion"
    )
    
    parser.add_argument(
        "--all", 
        action="store_true",
        help="Migrate all: universes, backtest results, and trade logs"
    )
    
    args = parser.parse_args()
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘      âš¡ðŸŒŸðŸ’Ž JSON to Parquet Migration ðŸ’ŽðŸŒŸâš¡                â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    input_dir = Path(args.input)
    output_dir = Path(args.output) if args.output else input_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if args.delete_json:
        print("âš ï¸  WARNING: JSON files will be deleted after successful conversion!")
        response = input("   Continue? (yes/no): ")
        if response.lower() != 'yes':
            print("   Aborted.")
            return 0
    
    total_stats = {
        'files_processed': 0,
        'total_size_before_mb': 0,
        'total_size_after_mb': 0
    }
    
    if args.all:
        # Migrate universes
        print("\n" + "="*63)
        print("ðŸ“¦ Migrating Universes")
        print("="*63)
        universe_dir = Path("ultra_necrozma_results/universes")
        if universe_dir.exists():
            stats = migrate_universes(universe_dir, universe_dir, args.delete_json)
            total_stats['files_processed'] += stats['files_processed']
            total_stats['total_size_before_mb'] += stats['total_size_before_mb']
            total_stats['total_size_after_mb'] += stats['total_size_after_mb']
        
        # Migrate backtest results
        print("\n" + "="*63)
        print("ðŸ“¦ Migrating Backtest Results")
        print("="*63)
        backtest_dir = Path("ultra_necrozma_results/backtest_results")
        if backtest_dir.exists():
            stats = migrate_backtest_results(backtest_dir, backtest_dir, args.delete_json)
            total_stats['files_processed'] += stats['files_processed']
            total_stats['total_size_before_mb'] += stats['total_size_before_mb']
            total_stats['total_size_after_mb'] += stats['total_size_after_mb']
        
        # Migrate trade logs
        print("\n" + "="*63)
        print("ðŸ“¦ Migrating Trade Logs")
        print("="*63)
        stats = migrate_trade_logs(backtest_dir, backtest_dir, args.delete_json)
        total_stats['files_processed'] += stats['files_processed']
        total_stats['total_size_before_mb'] += stats['total_size_before_mb']
        total_stats['total_size_after_mb'] += stats['total_size_after_mb']
        
    else:
        # Migrate specific type
        if args.type == 'universe':
            stats = migrate_universes(input_dir, output_dir, args.delete_json)
        elif args.type == 'backtest':
            stats = migrate_backtest_results(input_dir, output_dir, args.delete_json)
        elif args.type == 'trades':
            stats = migrate_trade_logs(input_dir.parent, output_dir.parent, args.delete_json)
        
        total_stats = stats
    
    # Print overall summary
    if total_stats['files_processed'] > 0:
        overall_savings = (1 - total_stats['total_size_after_mb']/total_stats['total_size_before_mb']) * 100
        print("\n" + "="*63)
        print("ðŸŽ‰ MIGRATION COMPLETE!")
        print("="*63)
        print(f"\nðŸ“Š Overall Statistics:")
        print(f"   Total files processed: {total_stats['files_processed']}")
        print(f"   Total size before: {total_stats['total_size_before_mb']:.1f}MB")
        print(f"   Total size after: {total_stats['total_size_after_mb']:.1f}MB")
        print(f"   Overall savings: {overall_savings:.0f}%")
        print(f"\nðŸ’¡ Parquet files are now ready for use!")
        print(f"   The system will automatically use Parquet when available.")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
